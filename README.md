## Kandityö: Transformer-mallit ja niiden soveltaminen aikasarjojen analyysiin

### Tiivistelmä

Transformer-arkkitehtuuri on mullistanut syväoppimisen monilla sovellusalueilla, kuten luonnollisen 
kielen ja kuvien käsittelyssä. Sen vahvuus perinteisiin neuroverkkoratkaisuihin verrattuna 
perustuu tehokkaaseen huomiomekanismiin, joka mahdollistaa tärkeiden riippuvuuksien
tunnistamisen syötteessä ilman tarvetta peräkkäiselle tiedonkäsittelylle. Transformer hyödyntää 
rinnakkaislaskentaa ja soveltuu erinomaisesti monimutkaisten ja pitkäkestoisten riippuvuuksien 
mallintamiseen, mikä antaa sille merkittävän edun muihin neuroverkkoratkaisuihin
verrattuna. Nämä ominaisuudet tekevät Transformer-malleista kiinnostavan vaihtoehdon myös
aikasarjojen analyysissä, jossa datan erityispiirteet, kuten trendit ja kausivaihtelut, asettavat
omat vaatimuksensa.
Tutkielmassa tarkastellaan ensin Transformer-arkkitehtuurin perusrakennetta ja sen keskeisiä
ominaisuuksia, kuten huomiomekanismia ja rinnakkaislaskennan mahdollistavia tekijöitä. Tämän 
jälkeen siirrytään aikasarjojen analyysiin, esittelemällä ensin aikasarjojen ominaispiirteet
sekä perinteiset menetelmät, joita on käytetty aikasarjojen mallintamisessa. Lisäksi käsitellään
erilaisia Transformer-pohjaisia ratkaisuja, joita on kehitetty aikasarjojen analysoimiseksi. 
Aikasarjat asettavat monia haasteita malleille ja arkkitehtuuria täytyy muokata näiden haasteiden
ratkaisemiseksi. Vaikka Transformer-mallien potentiaali aikasarja-analyysissä on käynyt ilmi,
niiden mahdollisuuksia ja rajoitteita ei vielä täysin ymmärretä. Transformer-mallien joustavuus
ja muokattavuus tarjoavat kuitenkin vahvan pohjan jatkotutkimukselle.

---

## Bachelor's thesis: Transformer Models and Their Application to Time Series Analysis

### Abstract in English

The Transformer architecture has revolutionized deep learning across various application domains, such as natural language and image processing. Its strength compared to traditional neural network solutions lies in its efficient attention mechanism, which enables the identification of key dependencies in the input without requiring sequential information processing. The Transformer leverages parallel computation and is exceptionally suited for modeling complex and long-term dependencies, giving it a significant advantage over other neural network solutions. These features make Transformer models an intriguing option for time series analysis, where the unique characteristics of the data, such as trends and seasonality, impose specific requirements.

This thesis first examines the fundamental structure of the Transformer architecture and its key features, such as the attention mechanism and the factors enabling parallel computation. Following this, the focus shifts to time series analysis, introducing the main characteristics of time series data and the traditional methods used for modeling them. Additionally, various Transformer-based solutions developed for analyzing time series are discussed. Time series present numerous challenges for models, requiring modifications to the architecture to address these issues. While the potential of Transformer models in time series analysis has become evident, their possibilities and limitations are not yet fully understood. Nevertheless, the flexibility and adaptability of Transformer models provide a solid foundation for further research.
